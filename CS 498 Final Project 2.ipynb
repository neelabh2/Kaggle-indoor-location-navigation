{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn as skl\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from dataclasses import dataclass\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create read_data_file which gives wifi data, waypoint data, and siteID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wifi shape: (5258, 5)\n",
      "waypoint shape: (6, 3)\n"
     ]
    }
   ],
   "source": [
    "# copy from https://github.com/location-competition/indoor-location-competition-20/blob/master/io_f.py\n",
    "\n",
    "@dataclass\n",
    "class ReadData:\n",
    "    wifi: np.ndarray\n",
    "    waypoint: np.ndarray\n",
    "    siteID: str\n",
    "\n",
    "def read_data_file(data_filename):\n",
    "    wifi = []\n",
    "    waypoint = []\n",
    "    siteID = ''\n",
    "\n",
    "    with open(data_filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line_data in lines:\n",
    "        \n",
    "        if 'SiteID:' in line_data:\n",
    "            siteID = line_data.split('\\t')[1].replace('SiteID:', '')\n",
    "\n",
    "        line_data = line_data.strip()\n",
    "        if not line_data or line_data[0] == '#':\n",
    "            continue\n",
    "\n",
    "        line_data = line_data.split('\\t')\n",
    "        \n",
    "        if line_data[1] == 'TYPE_WAYPOINT':\n",
    "            waypoint.append([int(line_data[0]), float(line_data[2]), float(line_data[3])])\n",
    "            continue\n",
    "        \n",
    "        if line_data[1] == 'TYPE_WIFI':\n",
    "            sys_ts = line_data[0]\n",
    "            ssid = line_data[2]\n",
    "            bssid = line_data[3]\n",
    "            rssi = line_data[4]\n",
    "            lastseen_ts = line_data[6]\n",
    "            wifi_data = [sys_ts, ssid, bssid, rssi, lastseen_ts]\n",
    "            wifi.append(wifi_data)\n",
    "            continue\n",
    "    \n",
    "    wifi = np.array(wifi)\n",
    "    waypoint = np.array(waypoint)\n",
    "    #print(wifi)\n",
    "    \n",
    "    return ReadData(wifi, waypoint, siteID)\n",
    "\n",
    "sample_file = read_data_file(\"/Users/neel2sharma/indoor-location-navigation/train/5a0546857ecc773753327266/F2/5dccf516c04f060006e6e3c9.txt\")\n",
    "\n",
    "print('wifi shape:', sample_file.wifi.shape)\n",
    "print('waypoint shape:', sample_file.waypoint.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the used buildings into used_buildings in order to remove unnecessary training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/devinanzelmo/wifi-features\n",
    "\n",
    "# pull out all the buildings actually used in the test set, given current method we don't need the other ones\n",
    "ssubm = pd.read_csv('/Users/neel2sharma/indoor-location-navigation/sample_submission.csv')\n",
    "\n",
    "# only 24 of the total buildings are used in the test set, \n",
    "# this allows us to greatly reduce the intial size of the dataset\n",
    "\n",
    "ssubm_df = ssubm[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n",
    "used_buildings = sorted(ssubm_df[0].value_counts().index.tolist())\n",
    "\n",
    "floor_map = {\"B2\":-2, \"B1\":-1, \"F1\":0, \"F2\": 1, \"F3\":2, \"F4\":3, \"F5\":4, \"F6\":5, \"F7\":6,\"F8\":7, \"F9\":8,\n",
    "             \"1F\":0, \"2F\":1, \"3F\":2, \"4F\":3, \"5F\":4, \"6F\":5, \"7F\":6, \"8F\": 7, \"9F\":8}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 24 different train_x and 24 of each train_yx, train_yy\n",
    "    \n",
    "Note siteID and filename (which is path) somewhere - in a map?\n",
    "    \n",
    "Figure out how to predict floor\n",
    "\n",
    "Create 24 models and train appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNBX1 = MultinomialNB()\n",
    "MNBX2 = MultinomialNB()\n",
    "MNBX3 = MultinomialNB()\n",
    "MNBX4 = MultinomialNB()\n",
    "MNBX5 = MultinomialNB()\n",
    "MNBX6 = MultinomialNB()\n",
    "MNBX7 = MultinomialNB()\n",
    "MNBX8 = MultinomialNB()\n",
    "MNBX9 = MultinomialNB()\n",
    "MNBX10 = MultinomialNB()\n",
    "MNBX11 = MultinomialNB()\n",
    "MNBX12 = MultinomialNB()\n",
    "MNBX13 = MultinomialNB()\n",
    "MNBX14 = MultinomialNB()\n",
    "MNBX15 = MultinomialNB()\n",
    "MNBX16 = MultinomialNB()\n",
    "MNBX17 = MultinomialNB()\n",
    "MNBX18 = MultinomialNB()\n",
    "MNBX19 = MultinomialNB()\n",
    "MNBX20 = MultinomialNB()\n",
    "MNBX21 = MultinomialNB()\n",
    "MNBX22 = MultinomialNB()\n",
    "MNBX23 = MultinomialNB()\n",
    "MNBX24 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNBY1 = MultinomialNB()\n",
    "MNBY2 = MultinomialNB()\n",
    "MNBY3 = MultinomialNB()\n",
    "MNBY4 = MultinomialNB()\n",
    "MNBY5 = MultinomialNB()\n",
    "MNBY6 = MultinomialNB()\n",
    "MNBY7 = MultinomialNB()\n",
    "MNBY8 = MultinomialNB()\n",
    "MNBY9 = MultinomialNB()\n",
    "MNBY10 = MultinomialNB()\n",
    "MNBY11 = MultinomialNB()\n",
    "MNBY12 = MultinomialNB()\n",
    "MNBY13 = MultinomialNB()\n",
    "MNBY14 = MultinomialNB()\n",
    "MNBY15 = MultinomialNB()\n",
    "MNBY16 = MultinomialNB()\n",
    "MNBY17 = MultinomialNB()\n",
    "MNBY18 = MultinomialNB()\n",
    "MNBY19 = MultinomialNB()\n",
    "MNBY20 = MultinomialNB()\n",
    "MNBY21 = MultinomialNB()\n",
    "MNBY22 = MultinomialNB()\n",
    "MNBY23 = MultinomialNB()\n",
    "MNBY24 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNBYY1 = MultinomialNB()\n",
    "MNBYY2 = MultinomialNB()\n",
    "MNBYY3 = MultinomialNB()\n",
    "MNBYY4 = MultinomialNB()\n",
    "MNBYY5 = MultinomialNB()\n",
    "MNBYY6 = MultinomialNB()\n",
    "MNBYY7 = MultinomialNB()\n",
    "MNBYY8 = MultinomialNB()\n",
    "MNBYY9 = MultinomialNB()\n",
    "MNBYY10 = MultinomialNB()\n",
    "MNBYY11 = MultinomialNB()\n",
    "MNBYY12 = MultinomialNB()\n",
    "MNBYY13 = MultinomialNB()\n",
    "MNBYY14 = MultinomialNB()\n",
    "MNBYY15 = MultinomialNB()\n",
    "MNBYY16 = MultinomialNB()\n",
    "MNBYY17 = MultinomialNB()\n",
    "MNBYY18 = MultinomialNB()\n",
    "MNBYY19 = MultinomialNB()\n",
    "MNBYY20 = MultinomialNB()\n",
    "MNBYY21 = MultinomialNB()\n",
    "MNBYY22 = MultinomialNB()\n",
    "MNBYY23 = MultinomialNB()\n",
    "MNBYY24 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have an array of MNBs - loop through each building and build train_x, train_yx, train_yy internally\n",
    "#then do MNBXS[i].fit(train_x, train_yx), MNBYS[i].fit(train_x, train_yy)\n",
    "\n",
    "#still need to figure out how to predict floor, how to link each model pair (MNBX1, MNBY1) to siteID (path is determined by the test file)\n",
    "    #linking is important because it tells me which model to use for each test file\n",
    "\n",
    "MNBXS = [MNBX1, MNBX2, MNBX3, MNBX4, MNBX5, MNBX6, MNBX7, MNBX8, MNBX9, MNBX10, MNBX11, MNBX12, MNBX13, MNBX14, MNBX15, MNBX16, MNBX17, MNBX18, MNBX19, MNBX20, MNBX21, MNBX22, MNBX23, MNBX24]\n",
    "MNBYS = [MNBY1, MNBY2, MNBY3, MNBY4, MNBY5, MNBY6, MNBY7, MNBY8, MNBY9, MNBY10, MNBY11, MNBY12, MNBY13, MNBY14, MNBY15, MNBY16, MNBY17, MNBY18, MNBY19, MNBY20, MNBY21, MNBY22, MNBY23, MNBX24]\n",
    "\n",
    "z = 0\n",
    "for building in used_buildings:\n",
    "\n",
    "    folders = sorted(glob.glob(os.path.join('/Users/neel2sharma/indoor-location-navigation/','train/'+building+'/*')))\n",
    "    print(building)\n",
    "\n",
    "    for folder in folders:\n",
    "        floor = floor_map[folder.split('/')[-1]]\n",
    "        files = glob.glob(os.path.join(folder, \"*.txt\"))\n",
    "\n",
    "        train_x = []\n",
    "        train_yx = []\n",
    "        train_yy = []\n",
    "        \n",
    "        for file in files:\n",
    "            #at this point, goes through each file - floor is stored in floor, siteID is stored in building, and path is file[80:104]\n",
    "            #Now fill train_x, train_yx, and train_yy as before\n",
    "            train_file = read_data_file(file)\n",
    "            \n",
    "            #obtain waypoint values\n",
    "            waypoint_values_x = train_file.waypoint[:,1]\n",
    "            waypoint_values_y = train_file.waypoint[:,2]\n",
    "            waypoint_timestamps = train_file.waypoint[:,0]\n",
    "\n",
    "            #Append x and y data for this file into train_yx and train_yy (lists rather than ndarrays for sklearn)\n",
    "            i = 0\n",
    "            while i < len(waypoint_values_x):\n",
    "                train_yx.append(waypoint_values_x[i].astype(int))\n",
    "                i += 1\n",
    "                \n",
    "            i = 0\n",
    "            while i < len(waypoint_values_y):\n",
    "                train_yy.append(waypoint_values_y[i].astype(int))\n",
    "                i += 1\n",
    "                \n",
    "            #Select closest timestamps\n",
    "            wifi_timestamps = [0] * len(waypoint_timestamps)\n",
    "            i = 0\n",
    "            while i < len(waypoint_timestamps):\n",
    "                for x in train_file.wifi:\n",
    "                    y = x[0].astype(float)\n",
    "                    if abs(waypoint_timestamps[i] - y) < abs(waypoint_timestamps[i] - wifi_timestamps[i]):\n",
    "                        wifi_timestamps[i] = y\n",
    "                i += 1\n",
    "\n",
    "\n",
    "            #create nparray of zeroes with len = 561 as toAppend. Instead of appending to toAppend, use i to update individual values\n",
    "            #this way all arrays in train_x have same length\n",
    "            i = 0\n",
    "            while i < len(wifi_timestamps):\n",
    "                toAppend = np.zeros(561)\n",
    "                for x in train_file.wifi:\n",
    "                    if wifi_timestamps[i].astype(float) == x[0].astype(float):\n",
    "                        if x[3] == 'nan':\n",
    "                            toAppend[i] = 0\n",
    "                        else:\n",
    "                            toAppend[i] = x[3].astype(int)\n",
    "                toAppend[i] = floor #may be out of bounds, maybe do 562\n",
    "                train_x.append(toAppend)\n",
    "                i += 1\n",
    "\n",
    "            list_train_x = np.array([np.array(xi) for xi in train_x])\n",
    "\n",
    "            MNBXS[z].fit(train_x, train_yx)\n",
    "            MNBYS[z].fit(train_x, train_yy)\n",
    "\n",
    "    z += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile test data into 24 different test_x\n",
    "\n",
    "Predict x and y\n",
    "\n",
    "Compile into csv for submission\n",
    "\n",
    "still need to figure out how to predict floor and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do test_x internally and store predictions in lists\n",
    "\n",
    "sites = [] #check siteID\n",
    "paths = [] #check file name\n",
    "x_predict = [] #do I need 24 of these?\n",
    "y_predict = []\n",
    "\n",
    "for building in used_buildings: #alter this to fit test data \n",
    "\n",
    "    folders = sorted(glob.glob(os.path.join('/Users/neel2sharma/indoor-location-navigation/','train/'+building+'/*')))\n",
    "    print(building)\n",
    "\n",
    "    for folder in folders:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('/Users/neel2sharma/indoor-location-navigation/test'):\n",
    "    for filename in filenames:\n",
    "        if (filename == \".DS_Store\"):\n",
    "            continue\n",
    "        train_file = read_data_file(os.path.join(dirname, filename))\n",
    "        \n",
    "       # if (train_file.siteID != '5c3c44b80379370013e0fd2b'):\n",
    "        #    continue\n",
    "        \n",
    "        #problems here: wifi_timestamps doesn't exist - I don't know how many of the rssi rows to take\n",
    "        #train_x looks like [[rssi11, rssi12, rssi13, rssi13, rssi14, rssi15, rssi16], [rssi21, rssi22, rssi23, rssi24, rssi25, rssi26], ...]\n",
    "            #in the current filter, the longest such sublist is 561 points long\n",
    "        i = 0\n",
    "        while i < 6: #make a prediction for each timestamp\n",
    "            toAppend = np.zeros(561) #I got 561 for the length of the longest rssi point\n",
    "            \n",
    "            if x[3] == 'nan':\n",
    "                toAppend[i] = 0\n",
    "            else:\n",
    "                toAppend[i] = x[3].astype(int)\n",
    "            test_x.append(toAppend)\n",
    "            i += 1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#bnb and lr are giving lists of lists of the same value (ex [[97, 101], [97, 101], [97, 101],...])\n",
    "#mnb is giving a 2D array where each diagonal line is repeats\n",
    "    #honestly not sure what's going on - test data once prof responds, set up meeting if it fails\n",
    "predicted_x = MNBX.predict(test_x)\n",
    "predicted_y = MNBY.predict(test_x)\n",
    "\n",
    "predicted_merge = []\n",
    "i = 0\n",
    "while (i < len(predicted_x)):\n",
    "    toAppend = []\n",
    "    toAppend.append(predicted_x[i])\n",
    "    toAppend.append(predicted_y[i])\n",
    "    predicted_merge.append(toAppend)\n",
    "    i += 1\n",
    "print(predicted_merge)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
